---
title: "Homework #7: Clustering" 
author: "**Jacqui Unciano**"
format: ds6030hw-html
---

::: {style="background-color:yellow; color:red; display: block; border-color: black; padding:1em"}
This is an **independent assignment**. Do not discuss or work with classmates.
:::

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```

# Required R packages and Directories {.unnumbered .unlisted}

```{r packages, message=FALSE, warning=FALSE}
data.dir = 'https://mdporter.github.io/DS6030/data/' # data directory
library(mclust)    # for model-based clustering
library(mixtools)  # for poisson mixture mode
library(tidyverse) # functions for data manipulation   
```

# Problem 1: Customer Segmentation with RFM (Recency, Frequency, and Monetary Value)

RFM analysis is an approach that some businesses use to understand their customers' activities. At any point in time, a company can measure how recently a customer purchased a product (Recency), how many times they purchased a product (Frequency), and how much they have spent (Monetary Value). There are many ad-hoc attempts to segment/cluster customers based on the RFM scores (e.g., here is one based on using the customers' rank of each dimension independently: <https://joaocorreia.io/blog/rfm-analysis-increase-sales-by-segmenting-your-customers.html>). In this problem you will use the clustering methods we covered in class to segment the customers.

The data for this problem can be found here: \<`r file.path(data.dir, "RFM.csv")`\>. Cluster based on the Recency, Frequency, and Monetary value columns.

::: {.callout-note title="Solution"}
```{r}
data = read.csv("C:/Users/jacqu/Downloads/RFM.csv")
head(data)
```
:::

## a. Implement hierarchical clustering.

-   Describe any pre-processing steps you took (e.g., scaling, distance metric)
-   State the linkage method you used with justification.
-   Show the resulting dendrogram
-   State the number of segments/clusters you used with justification.
-   Using your segmentation, are customers 1 and 100 in the same cluster?

::: {.callout-note title="Solution"}
```{r}
## scaling the data
dat = data %>%
  select(Recency, Frequency, Monetary) %>%
  scale() %>%
  as_tibble()
head(dat)
```

```{r}
## distance calulated with euclidean method, and centroid approach with the number of 
## points taken into consideration (aka ward)
dis = dist(dat, method="euclidean")
hc = hclust(dis, method="ward.D2")
```

```{r}
tibble(height = log(hc$height), K = row_number(-height)) %>%
  ggplot(aes(K, height)) +
  geom_line() +
  geom_point(aes(color = ifelse(K == 9, "red", "black"))) +
  scale_color_identity() +
  coord_cartesian(xlim=c(1, 50), y=c(2, NA)) +
  labs(y = "log height")
```

```{r}
cutree(hc, k=9) %>% table()
```

```{r}
print(cutree(hc, k=9)[1])
print(cutree(hc, k=9)[100])
```

I first scaled the data and then used the ward linkage method because it takes into account the number of points in each cluster, which I like and I think is better than simple centroid approach. Because of the elbow plot, I chose K=9, which makes obs 1 and 100 not in the same cluster.
:::

## b. Implement k-means.

-   Describe any pre-processing steps you took (e.g., scaling)
-   State the number of segments/clusters you used with justification.
-   Using your segmentation, are customers 1 and 100 in the same cluster?

::: {.callout-note title="Solution"}
```{r}
Kmax = 20
SSE = numeric(Kmax) 
for(k in 1:Kmax){
  km = kmeans(dat, centers=k, nstart=25) 
  SSE[k] = km$tot.withinss # get SSE
}
results = tibble(K = 1:Kmax, SSE = log(SSE))

results %>% ggplot(aes(K, log(SSE))) +
  geom_point() + geom_line() +
  scale_x_continuous(breaks=seq(0, 100, by=2))
```

```{r}
km = kmeans(dat, centers=9, nstart=25) # choose K=9
table(est=km$cluster)
```

```{r}
print(km$cluster[1])
print(km$cluster[100])
```

I first scaled the data and then used the ward linkage method because it takes into account the number of points in each cluster, which I like and I think is better than simple centroid approach. Because the plot didn't have an obvious elbow in it, I chose K=9 based on the other plot, which makes obs 1 and 100 not in the same cluster.
:::

## c. Implement model-based clustering

-   Describe any pre-processing steps you took (e.g., scaling)
-   State the number of segments/clusters you used with justification.
-   Describe the best model. What restrictions are on the shape of the components?
-   Using your segmentation, are customers 1 and 100 in the same cluster?

::: {.callout-note title="Solution"}
```{r}
mix = Mclust(dat, verbose=FALSE) 
summary(mix) 
```

```{r}
plot(mix, what="BIC") 
plot(mix, what="classification")
plot(mix, what="uncertainty")
plot(mix, what="density")
```

```{r}
print(mix$classification[1])
print(mix$classification[100])
```

I first scaled the data and then used the ward linkage method because it takes into account the number of points in each cluster, which I like and I think is better than simple centroid approach. Because of the summary output, I chose K=8, which makes obs 1 and 100 not in the same cluster. And it looks like the chosen model is VVE (ellipsoidal, equal orientation) model.
:::

## d. Discussion of results

Discuss how you would cluster the customers if you had to do this for your job. Do you think one model would do better than the others?

::: {.callout-note title="Solution"}
I don't know if this is possible, but I would cluster it with a 3D model. Right now, it seems as though the clusters are based on a pairing RF and FM and MR, but I would like to see clustering based on RFM, which I think would be a better model to see which customers are super loyal.

I do think that if we're wondering which customers just spend the most money and frequently/moderately frequently would be a good way to cluster the data too. In this case, I think 8-9 clusters would be good because it's supported by both the hierarchical clustering method and the model-based clustering method.
:::

# Problem 2: Poisson Mixture Model

The pmf of a Poisson random variable is: \begin{align*}
f_k(x; \lambda_k) = \frac{\lambda_k^x e^{-\lambda_k}}{x!}
\end{align*}

A two-component Poisson mixture model can be written: \begin{align*}
f(x; \theta) = \pi \frac{\lambda_1^x e^{-\lambda_1}}{x!} + (1-\pi) \frac{\lambda_2^x e^{-\lambda_2}}{x!}
\end{align*}

## a. Model parameters

What are the parameters of the model?

::: {.callout-note title="Solution"}
$\theta = (\lambda_1, \lambda_2, \pi)$
:::

## b. Log-likelihood

Write down the log-likelihood for $n$ independent observations ($x_1, x_2, \ldots, x_n$).

::: {.callout-note title="Solution"}
$f(\Sigma x_i; \theta) = \pi \frac{\lambda_1^\Sigma x_i e^{-\lambda_1}}{\Sigma x_i!} + (1-\pi) \frac{\lambda_2^\Sigma x_i e^{-\lambda_2}}{\Sigma x_i!}$ $lnL(\theta) = \Sigma_{i=1}^{n}ln(\pi \frac{\lambda_1^\Sigma x_i e^{-\lambda_1}}{\Sigma x_i!} + (1-\pi) \frac{\lambda_2^\Sigma x_i e^{-\lambda_2}}{\Sigma x_i!})$
:::

## c. Updating the responsibilities

Suppose we have initial values of the parameters. Write down the equation for updating the *responsibilities*.

::: {.callout-note title="Solution"}
$r_{ik} = \frac{P(X|K=k, \theta_k) \pi_k}{\Sigma_{j=1}P(X|K=j, \theta_k) \pi_j}$
:::

## d. Updating the model parameters

Suppose we have responsibilities, $r_{ik}$ for all $i=1, 2, \ldots, n$ and $k=1,2$. Write down the equations for updating the parameters.

::: {.callout-note title="Solution"}
$r_{i1} = \frac{P(X_i|K=1, \theta_1) \pi_1}{P(X_i|K=1, \theta_1) \pi_1 + P(X_i|K=2, \theta_2) \pi_2}$ $r_{i2} = \frac{P(X_i|K=2, \theta_2) \pi_2}{P(X_i|K=1, \theta_1) \pi_1 + P(X_i|K=2, \theta_2) \pi_2}$
:::

## e. Fit a two-component Poisson mixture model

Fit a two-component Poisson mixture model. Report the estimated parameter values and show a plot of the estimated mixture pmf for the following data:

```{r, echo=TRUE}
#-- Run this code to generate the data
set.seed(123)             # set seed for reproducibility
n = 200                   # sample size
z = sample(1:2, size=n, replace=TRUE, prob=c(.25, .75)) # sample the latent class
theta = c(8, 16)          # true parameters
y = ifelse(z==1, rpois(n, lambda=theta[1]), rpois(n, lambda=theta[2]))
```

-   Note: The function `poisregmixEM()` in the R package `mixtools` is designed to estimate a mixture of *Poisson regression* models. We can still use this function for our problem of pmf estimation if it is recast as an intercept-only regression. To do so, set the $x$ argument (predictors) to `x = rep(1, length(y))` and `addintercept = FALSE`.
    -   Look carefully at the output from this model. The `beta` values (regression coefficients) are on the log scale.

::: {.callout-note title="Solution"}
```{r}
set.seed(123)
x = rep(1, length(y))
#-- Fit K=2 component mixture model
PMM = mixtools::poisregmixEM(x=x, y=y, addintercept = FALSE, k=2) # use poisregmixEM()
```

\``{R} (betas = PMM$beta) (lambdas = PMM$lambda)`

```{r}
#-- Plot 95% contour
plot(PMM$posterior, density=TRUE, alpha=0.05)
```
:::

## f. **2 pts Extra Credit** EM from scratch

Write a function that estimates this two-component Poisson mixture model using the EM approach. Show that it gives the same result as part *e*.

-   Note: you are not permitted to copy code. Write everything from scratch and use comments to indicate how the code works (e.g., the E-step, M-step, initialization strategy, and convergence should be clear).
-   Cite any resources you consulted to help with the coding.

::: {.callout-note title="Solution"}
Add solution here
:::
